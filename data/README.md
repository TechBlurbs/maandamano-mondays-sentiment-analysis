### The data is labeled by multilingual XLM-roBERTa-base model 

trained on ~198M tweets and fine-tuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) <br>
The model is : "cardiffnlp/twitter-xlm-roberta-base-sentiment" and hosted on huggingface

Labels in this order in the list of labels: <br>
&emsp;&emsp;{0: 'negative', 1: 'neutral', 2: 'positive'}


@inproceedings{barbieri-etal-2022-xlm,<br>
&emsp;&emsp;title = "{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond",<br>
&emsp;&emsp;author = "Barbieri, Francesco  and<br>
&emsp;&emsp;Espinosa Anke, Luis  and<br>
&emsp;&emsp;Camacho-Collados, Jose",<br>
&emsp;&emsp;booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",<br>
&emsp;&emsp;month = jun,<br>
&emsp;&emsp;year = "2022",<br>
&emsp;&emsp;address = "Marseille, France",<br>
&emsp;&emsp;publisher = "European Language Resources Association",<br>
&emsp;&emsp;url = "https://aclanthology.org/2022.lrec-1.27",<br>
&emsp;&emsp;pages = "258--266"<br>
&emsp;}
